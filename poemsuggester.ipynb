{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OoB9FN-yjmfP"
      },
      "source": [
        "First, import the libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hSxluof1iJv1"
      },
      "outputs": [],
      "source": [
        "!pip install torch\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VXjDjZPLjsJk"
      },
      "source": [
        "Import and optimize the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rnuvpffYi0wX"
      },
      "outputs": [],
      "source": [
        "# Read the dataset from the file\n",
        "with open('data.txt', 'r') as file:\n",
        "    poems = file.readlines()\n",
        "\n",
        "# Tokenize the poems\n",
        "tokens = []\n",
        "for poem in poems:\n",
        "    # Preprocess the text by removing special characters and symbols\n",
        "    processed_poem = poem.lower().strip().replace(\".\", \"\")\n",
        "    tokens += processed_poem.split()\n",
        "\n",
        "# Create a vocabulary\n",
        "vocab = list(set(tokens))\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "# Create word-to-index and index-to-word mappings\n",
        "word_to_idx = {word: i for i, word in enumerate(vocab)}\n",
        "idx_to_word = {i: word for i, word in enumerate(vocab)}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZJEOJERujysY"
      },
      "source": [
        "Preprocess the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k8TI62RXjDji"
      },
      "outputs": [],
      "source": [
        "# Set the window size\n",
        "window_size = 5\n",
        "\n",
        "# Generate training examples\n",
        "input_seqs = []\n",
        "target_seqs = []\n",
        "for i in range(len(tokens) - window_size):\n",
        "    input_seq = tokens[i:i+window_size]\n",
        "    target_seq = tokens[i+window_size]\n",
        "    input_seqs.append(input_seq)\n",
        "    target_seqs.append(target_seq)\n",
        "\n",
        "# Handle the last sequence that is shorter than the window size\n",
        "if len(tokens) >= window_size:\n",
        "    input_seq = tokens[-window_size:]\n",
        "    target_seq = tokens[-1]\n",
        "    input_seqs.append(input_seq)\n",
        "    target_seqs.append(target_seq)\n",
        "\n",
        "# Convert sequences to tensors\n",
        "input_tensors = []\n",
        "target_tensors = []\n",
        "for input_seq, target_seq in zip(input_seqs, target_seqs):\n",
        "    input_tensors.append(torch.tensor([word_to_idx[word] for word in input_seq], dtype=torch.long))\n",
        "    target_tensors.append(torch.tensor(word_to_idx[target_seq], dtype=torch.long))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WtUXBqRbj3Dq"
      },
      "source": [
        "Define the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1YSmLgCBjJ-Y"
      },
      "outputs": [],
      "source": [
        "class AutocompleteModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim):\n",
        "        super(AutocompleteModel, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        embedded = self.embedding(x)\n",
        "        output, _ = self.lstm(embedded)\n",
        "        output = self.fc(output[:, -1, :])\n",
        "        return output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yA672nN-j5Qq"
      },
      "source": [
        "Training (takes the most of  the time)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YtSPV--bjL8q"
      },
      "outputs": [],
      "source": [
        "# Set the hyperparameters\n",
        "embedding_dim = 128\n",
        "hidden_dim = 256\n",
        "epochs = 100\n",
        "batch_size = 64\n",
        "learning_rate = 0.001\n",
        "\n",
        "# Create the model\n",
        "model = AutocompleteModel(vocab_size, embedding_dim, hidden_dim)\n",
        "\n",
        "# Define the loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Split the data into training and validation sets\n",
        "train_ratio = 0.9\n",
        "train_size = int(train_ratio * len(input_tensors))\n",
        "train_inputs, val_inputs = input_tensors[:train_size], input_tensors[train_size:]\n",
        "train_targets, val_targets = target_tensors[:train_size], target_tensors[train_size:]\n",
        "\n",
        "# Train the model\n",
        "best_val_loss = float('inf')\n",
        "best_model_state_dict = None\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for i in range(0, len(train_inputs), batch_size):\n",
        "        batch_inputs = train_inputs[i:i+batch_size]\n",
        "        batch_targets = train_targets[i:i+batch_size]\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(torch.stack(batch_inputs))\n",
        "        loss = criterion(outputs, torch.stack(batch_targets))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    avg_train_loss = total_loss / len(train_inputs)\n",
        "\n",
        "    # Evaluate on the validation set\n",
        "    model.eval()\n",
        "    val_loss = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i in range(0, len(val_inputs), batch_size):\n",
        "            batch_inputs = val_inputs[i:i+batch_size]\n",
        "            batch_targets = val_targets[i:i+batch_size]\n",
        "\n",
        "            outputs = model(torch.stack(batch_inputs))\n",
        "            loss = criterion(outputs, torch.stack(batch_targets))\n",
        "            val_loss += loss.item()\n",
        "\n",
        "        avg_val_loss = val_loss / len(val_inputs)\n",
        "\n",
        "    if avg_val_loss < best_val_loss:\n",
        "        best_val_loss = avg_val_loss\n",
        "        best_model_state_dict = model.state_dict()\n",
        "\n",
        "    if (epoch + 1) % 1 == 0:\n",
        "        print(f\"Epoch: {epoch+1}/{epochs}, Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n",
        "\n",
        "# Load the best model state\n",
        "model.load_state_dict(best_model_state_dict)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "igRrIMSnj6jn"
      },
      "source": [
        "Run the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wAbPqMAfjNwm"
      },
      "outputs": [],
      "source": [
        "#@title Settings\n",
        "maxLengthForm = 1 #@param {type:\"integer\"}\n",
        "seed = 124 #@param {type:\"slider\", min:1, max:2500, step:1}\n",
        "suggestFor = \"dreams come\" #@param {type:\"string\"}\n",
        "# Set the model in evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Generate autocompletions\n",
        "input_sequence = suggestFor.lower()\n",
        "max_length = maxLengthForm\n",
        "beam_width = seed\n",
        "\n",
        "\n",
        "def score_beam_candidates(beam_candidates):\n",
        "    scores = []\n",
        "    for candidate in beam_candidates:\n",
        "        candidate_tensor = torch.tensor([word_to_idx[word] for word in candidate], dtype=torch.long).unsqueeze(0)\n",
        "        with torch.no_grad():\n",
        "            output = model(candidate_tensor)\n",
        "            score = torch.log_softmax(output, dim=1).sum()\n",
        "        scores.append(score.item())\n",
        "    return torch.tensor(scores)\n",
        "\n",
        "\n",
        " \n",
        "with torch.no_grad():\n",
        "    # Tokenize the input sequence\n",
        "    input_tokens = input_sequence.lower().split()\n",
        "    \n",
        "    # Filter out words that are not in the vocabulary\n",
        "    input_tokens = [token for token in input_tokens if token in vocab]\n",
        "    \n",
        "    # Check if the input sequence is empty after filtering\n",
        "    if len(input_tokens) == 0:\n",
        "        print(\"No valid words in the input sequence. Please try again with valid words.\")\n",
        "        exit()\n",
        "    \n",
        "    input_tensor = torch.tensor([word_to_idx[word] for word in input_tokens], dtype=torch.long).unsqueeze(0)\n",
        "\n",
        "    # Generate autocompletions using beam search\n",
        "    output_sequence = input_tokens[:]\n",
        "    for _ in range(max_length):\n",
        "        output = model(input_tensor)\n",
        "        _, topk_indices = torch.topk(output, beam_width, dim=1)\n",
        "\n",
        "        beam_candidates = []\n",
        "        for idx in topk_indices[0]:\n",
        "            predicted_word = idx_to_word[idx.item()]\n",
        "            beam_candidates.append(output_sequence + [predicted_word])\n",
        "\n",
        "        scores = score_beam_candidates(beam_candidates)\n",
        "        topk_scores, topk_indices = torch.topk(scores, beam_width)\n",
        "\n",
        "        output_sequence = beam_candidates[topk_indices[0].item()]\n",
        "        input_tensor = torch.tensor([word_to_idx[word] for word in output_sequence], dtype=torch.long).unsqueeze(0)\n",
        "\n",
        "    autocompletion = ' '.join(output_sequence)\n",
        "    print(f\"Suggestion: {autocompletion}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
