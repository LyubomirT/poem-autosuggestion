{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OoB9FN-yjmfP"
      },
      "source": [
        "Import the libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hSxluof1iJv1",
        "outputId": "19f8eda2-0fce-456d-8985-8581fe8c6100"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.0.1+cu118)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.12.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (16.0.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install torch\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VXjDjZPLjsJk"
      },
      "source": [
        "Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rnuvpffYi0wX"
      },
      "outputs": [],
      "source": [
        "# Read the dataset from the file\n",
        "with open('data.txt', 'r') as file:\n",
        "    poems = file.readlines()\n",
        "\n",
        "# Tokenize the poems\n",
        "tokens = []\n",
        "for poem in poems:\n",
        "    # Preprocess the text by removing special characters and symbols\n",
        "    processed_poem = poem.lower().strip().replace(\".\", \"\")\n",
        "    tokens += processed_poem.split()\n",
        "\n",
        "# Create a vocabulary\n",
        "vocab = list(set(tokens))\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "# Create word-to-index and index-to-word mappings\n",
        "word_to_idx = {word: i for i, word in enumerate(vocab)}\n",
        "idx_to_word = {i: word for i, word in enumerate(vocab)}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZJEOJERujysY"
      },
      "source": [
        "Preprocess"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k8TI62RXjDji"
      },
      "outputs": [],
      "source": [
        "# Set the window size\n",
        "window_size = 5\n",
        "\n",
        "# Generate training examples\n",
        "input_seqs = []\n",
        "target_seqs = []\n",
        "for i in range(len(tokens) - window_size):\n",
        "    input_seq = tokens[i:i+window_size]\n",
        "    target_seq = tokens[i+window_size]\n",
        "    input_seqs.append(input_seq)\n",
        "    target_seqs.append(target_seq)\n",
        "\n",
        "# Handle the last sequence that is shorter than the window size\n",
        "if len(tokens) >= window_size:\n",
        "    input_seq = tokens[-window_size:]\n",
        "    target_seq = tokens[-1]\n",
        "    input_seqs.append(input_seq)\n",
        "    target_seqs.append(target_seq)\n",
        "\n",
        "# Convert sequences to tensors\n",
        "input_tensors = []\n",
        "target_tensors = []\n",
        "for input_seq, target_seq in zip(input_seqs, target_seqs):\n",
        "    input_tensors.append(torch.tensor([word_to_idx[word] for word in input_seq], dtype=torch.long))\n",
        "    target_tensors.append(torch.tensor(word_to_idx[target_seq], dtype=torch.long))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WtUXBqRbj3Dq"
      },
      "source": [
        "Define"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1YSmLgCBjJ-Y"
      },
      "outputs": [],
      "source": [
        "class AutocompleteModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim):\n",
        "        super(AutocompleteModel, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        embedded = self.embedding(x)\n",
        "        output, _ = self.lstm(embedded)\n",
        "        output = self.fc(output[:, -1, :])\n",
        "        return output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yA672nN-j5Qq"
      },
      "source": [
        "Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YtSPV--bjL8q",
        "outputId": "9448423b-f805-4687-e8c0-3b295cc02af2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1/100, Train Loss: 0.1017, Val Loss: 0.1031\n",
            "Epoch: 2/100, Train Loss: 0.0836, Val Loss: 0.1036\n",
            "Epoch: 3/100, Train Loss: 0.0745, Val Loss: 0.1045\n",
            "Epoch: 4/100, Train Loss: 0.0653, Val Loss: 0.1066\n",
            "Epoch: 5/100, Train Loss: 0.0561, Val Loss: 0.1078\n",
            "Epoch: 6/100, Train Loss: 0.0466, Val Loss: 0.1098\n",
            "Epoch: 7/100, Train Loss: 0.0381, Val Loss: 0.1115\n",
            "Epoch: 8/100, Train Loss: 0.0312, Val Loss: 0.1141\n",
            "Epoch: 9/100, Train Loss: 0.0252, Val Loss: 0.1150\n",
            "Epoch: 10/100, Train Loss: 0.0196, Val Loss: 0.1170\n",
            "Epoch: 11/100, Train Loss: 0.0153, Val Loss: 0.1192\n",
            "Epoch: 12/100, Train Loss: 0.0121, Val Loss: 0.1204\n",
            "Epoch: 13/100, Train Loss: 0.0095, Val Loss: 0.1219\n",
            "Epoch: 14/100, Train Loss: 0.0073, Val Loss: 0.1238\n",
            "Epoch: 15/100, Train Loss: 0.0055, Val Loss: 0.1262\n",
            "Epoch: 16/100, Train Loss: 0.0041, Val Loss: 0.1280\n",
            "Epoch: 17/100, Train Loss: 0.0032, Val Loss: 0.1298\n",
            "Epoch: 18/100, Train Loss: 0.0026, Val Loss: 0.1317\n",
            "Epoch: 19/100, Train Loss: 0.0022, Val Loss: 0.1328\n",
            "Epoch: 20/100, Train Loss: 0.0019, Val Loss: 0.1341\n",
            "Epoch: 21/100, Train Loss: 0.0017, Val Loss: 0.1350\n",
            "Epoch: 22/100, Train Loss: 0.0016, Val Loss: 0.1360\n",
            "Epoch: 23/100, Train Loss: 0.0015, Val Loss: 0.1374\n",
            "Epoch: 24/100, Train Loss: 0.0014, Val Loss: 0.1385\n",
            "Epoch: 25/100, Train Loss: 0.0013, Val Loss: 0.1393\n",
            "Epoch: 26/100, Train Loss: 0.0013, Val Loss: 0.1403\n",
            "Epoch: 27/100, Train Loss: 0.0012, Val Loss: 0.1414\n",
            "Epoch: 28/100, Train Loss: 0.0012, Val Loss: 0.1427\n",
            "Epoch: 29/100, Train Loss: 0.0011, Val Loss: 0.1436\n",
            "Epoch: 30/100, Train Loss: 0.0012, Val Loss: 0.1437\n",
            "Epoch: 31/100, Train Loss: 0.0011, Val Loss: 0.1439\n",
            "Epoch: 32/100, Train Loss: 0.0011, Val Loss: 0.1449\n",
            "Epoch: 33/100, Train Loss: 0.0011, Val Loss: 0.1456\n",
            "Epoch: 34/100, Train Loss: 0.0010, Val Loss: 0.1472\n",
            "Epoch: 35/100, Train Loss: 0.0010, Val Loss: 0.1494\n",
            "Epoch: 36/100, Train Loss: 0.0010, Val Loss: 0.1491\n",
            "Epoch: 37/100, Train Loss: 0.0009, Val Loss: 0.1484\n",
            "Epoch: 38/100, Train Loss: 0.0009, Val Loss: 0.1476\n",
            "Epoch: 39/100, Train Loss: 0.0009, Val Loss: 0.1503\n",
            "Epoch: 40/100, Train Loss: 0.0009, Val Loss: 0.1508\n",
            "Epoch: 41/100, Train Loss: 0.0009, Val Loss: 0.1506\n",
            "Epoch: 42/100, Train Loss: 0.0009, Val Loss: 0.1501\n",
            "Epoch: 43/100, Train Loss: 0.0009, Val Loss: 0.1506\n",
            "Epoch: 44/100, Train Loss: 0.0009, Val Loss: 0.1519\n",
            "Epoch: 45/100, Train Loss: 0.0008, Val Loss: 0.1525\n",
            "Epoch: 46/100, Train Loss: 0.0008, Val Loss: 0.1518\n",
            "Epoch: 47/100, Train Loss: 0.0008, Val Loss: 0.1516\n",
            "Epoch: 48/100, Train Loss: 0.0008, Val Loss: 0.1518\n",
            "Epoch: 49/100, Train Loss: 0.0007, Val Loss: 0.1526\n",
            "Epoch: 50/100, Train Loss: 0.0007, Val Loss: 0.1529\n",
            "Epoch: 51/100, Train Loss: 0.0007, Val Loss: 0.1526\n",
            "Epoch: 52/100, Train Loss: 0.0007, Val Loss: 0.1528\n",
            "Epoch: 53/100, Train Loss: 0.0007, Val Loss: 0.1538\n",
            "Epoch: 54/100, Train Loss: 0.0007, Val Loss: 0.1538\n",
            "Epoch: 55/100, Train Loss: 0.0007, Val Loss: 0.1543\n",
            "Epoch: 56/100, Train Loss: 0.0008, Val Loss: 0.1538\n",
            "Epoch: 57/100, Train Loss: 0.0008, Val Loss: 0.1545\n",
            "Epoch: 58/100, Train Loss: 0.0008, Val Loss: 0.1557\n",
            "Epoch: 59/100, Train Loss: 0.0007, Val Loss: 0.1561\n",
            "Epoch: 60/100, Train Loss: 0.0007, Val Loss: 0.1567\n",
            "Epoch: 61/100, Train Loss: 0.0007, Val Loss: 0.1566\n",
            "Epoch: 62/100, Train Loss: 0.0007, Val Loss: 0.1559\n",
            "Epoch: 63/100, Train Loss: 0.0007, Val Loss: 0.1564\n",
            "Epoch: 64/100, Train Loss: 0.0007, Val Loss: 0.1573\n",
            "Epoch: 65/100, Train Loss: 0.0007, Val Loss: 0.1574\n",
            "Epoch: 66/100, Train Loss: 0.0007, Val Loss: 0.1583\n",
            "Epoch: 67/100, Train Loss: 0.0007, Val Loss: 0.1577\n",
            "Epoch: 68/100, Train Loss: 0.0007, Val Loss: 0.1586\n",
            "Epoch: 69/100, Train Loss: 0.0007, Val Loss: 0.1595\n",
            "Epoch: 70/100, Train Loss: 0.0007, Val Loss: 0.1591\n",
            "Epoch: 71/100, Train Loss: 0.0007, Val Loss: 0.1587\n",
            "Epoch: 72/100, Train Loss: 0.0007, Val Loss: 0.1604\n",
            "Epoch: 73/100, Train Loss: 0.0007, Val Loss: 0.1597\n",
            "Epoch: 74/100, Train Loss: 0.0007, Val Loss: 0.1599\n",
            "Epoch: 75/100, Train Loss: 0.0007, Val Loss: 0.1601\n",
            "Epoch: 76/100, Train Loss: 0.0007, Val Loss: 0.1614\n",
            "Epoch: 77/100, Train Loss: 0.0007, Val Loss: 0.1614\n",
            "Epoch: 78/100, Train Loss: 0.0007, Val Loss: 0.1603\n",
            "Epoch: 79/100, Train Loss: 0.0007, Val Loss: 0.1616\n",
            "Epoch: 80/100, Train Loss: 0.0007, Val Loss: 0.1576\n",
            "Epoch: 81/100, Train Loss: 0.0022, Val Loss: 0.1550\n",
            "Epoch: 82/100, Train Loss: 0.0016, Val Loss: 0.1566\n",
            "Epoch: 83/100, Train Loss: 0.0008, Val Loss: 0.1595\n",
            "Epoch: 84/100, Train Loss: 0.0007, Val Loss: 0.1601\n",
            "Epoch: 85/100, Train Loss: 0.0007, Val Loss: 0.1608\n",
            "Epoch: 86/100, Train Loss: 0.0007, Val Loss: 0.1613\n",
            "Epoch: 87/100, Train Loss: 0.0006, Val Loss: 0.1618\n",
            "Epoch: 88/100, Train Loss: 0.0006, Val Loss: 0.1622\n",
            "Epoch: 89/100, Train Loss: 0.0006, Val Loss: 0.1626\n",
            "Epoch: 90/100, Train Loss: 0.0006, Val Loss: 0.1629\n",
            "Epoch: 91/100, Train Loss: 0.0006, Val Loss: 0.1632\n",
            "Epoch: 92/100, Train Loss: 0.0006, Val Loss: 0.1635\n",
            "Epoch: 93/100, Train Loss: 0.0006, Val Loss: 0.1639\n",
            "Epoch: 94/100, Train Loss: 0.0006, Val Loss: 0.1641\n",
            "Epoch: 95/100, Train Loss: 0.0006, Val Loss: 0.1644\n",
            "Epoch: 96/100, Train Loss: 0.0006, Val Loss: 0.1648\n",
            "Epoch: 97/100, Train Loss: 0.0006, Val Loss: 0.1651\n",
            "Epoch: 98/100, Train Loss: 0.0006, Val Loss: 0.1651\n",
            "Epoch: 99/100, Train Loss: 0.0006, Val Loss: 0.1657\n",
            "Epoch: 100/100, Train Loss: 0.0006, Val Loss: 0.1660\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "# Set the hyperparameters\n",
        "embedding_dim = 128\n",
        "hidden_dim = 256\n",
        "epochs = 100\n",
        "batch_size = 64\n",
        "learning_rate = 0.001\n",
        "\n",
        "# Create the model\n",
        "model = AutocompleteModel(vocab_size, embedding_dim, hidden_dim)\n",
        "\n",
        "# Define the loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Split the data into training and validation sets\n",
        "train_ratio = 0.9\n",
        "train_size = int(train_ratio * len(input_tensors))\n",
        "train_inputs, val_inputs = input_tensors[:train_size], input_tensors[train_size:]\n",
        "train_targets, val_targets = target_tensors[:train_size], target_tensors[train_size:]\n",
        "\n",
        "# Train the model\n",
        "best_val_loss = float('inf')\n",
        "best_model_state_dict = None\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for i in range(0, len(train_inputs), batch_size):\n",
        "        batch_inputs = train_inputs[i:i+batch_size]\n",
        "        batch_targets = train_targets[i:i+batch_size]\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(torch.stack(batch_inputs))\n",
        "        loss = criterion(outputs, torch.stack(batch_targets))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    avg_train_loss = total_loss / len(train_inputs)\n",
        "\n",
        "    # Evaluate on the validation set\n",
        "    model.eval()\n",
        "    val_loss = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i in range(0, len(val_inputs), batch_size):\n",
        "            batch_inputs = val_inputs[i:i+batch_size]\n",
        "            batch_targets = val_targets[i:i+batch_size]\n",
        "\n",
        "            outputs = model(torch.stack(batch_inputs))\n",
        "            loss = criterion(outputs, torch.stack(batch_targets))\n",
        "            val_loss += loss.item()\n",
        "\n",
        "        avg_val_loss = val_loss / len(val_inputs)\n",
        "\n",
        "    if avg_val_loss < best_val_loss:\n",
        "        best_val_loss = avg_val_loss\n",
        "        best_model_state_dict = model.state_dict()\n",
        "\n",
        "    if (epoch + 1) % 1 == 0:\n",
        "        print(f\"Epoch: {epoch+1}/{epochs}, Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n",
        "\n",
        "# Load the best model state\n",
        "model.load_state_dict(best_model_state_dict)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "igRrIMSnj6jn"
      },
      "source": [
        "Run the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wAbPqMAfjNwm",
        "outputId": "4c12ca94-2c6f-452f-c120-85b0cb89e7a0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Suggestion: i find that are\n"
          ]
        }
      ],
      "source": [
        "#@title Settings\n",
        "maxLengthForm = 1 #@param {type:\"integer\"}\n",
        "seed = 124 #@param {type:\"slider\", min:1, max:10000, step:1}\n",
        "suggestFor = \"whenever i find out that\" #@param {type:\"string\"}\n",
        "# Set the model in evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Generate autocompletions\n",
        "input_sequence = suggestFor.lower()\n",
        "max_length = maxLengthForm\n",
        "beam_width = seed\n",
        "\n",
        "\n",
        "def score_beam_candidates(beam_candidates):\n",
        "    scores = []\n",
        "    for candidate in beam_candidates:\n",
        "        candidate_tensor = torch.tensor([word_to_idx[word] for word in candidate], dtype=torch.long).unsqueeze(0)\n",
        "        with torch.no_grad():\n",
        "            output = model(candidate_tensor)\n",
        "            score = torch.log_softmax(output, dim=1).sum()\n",
        "        scores.append(score.item())\n",
        "    return torch.tensor(scores)\n",
        "\n",
        "\n",
        " \n",
        "with torch.no_grad():\n",
        "    # Tokenize the input sequence\n",
        "    input_tokens = input_sequence.lower().split()\n",
        "    \n",
        "    # Filter out words that are not in the vocabulary\n",
        "    input_tokens = [token for token in input_tokens if token in vocab]\n",
        "    \n",
        "    # Check if the input sequence is empty after filtering\n",
        "    if len(input_tokens) == 0:\n",
        "        print(\"No valid words in the input sequence. Please try again with valid words.\")\n",
        "        exit()\n",
        "    \n",
        "    input_tensor = torch.tensor([word_to_idx[word] for word in input_tokens], dtype=torch.long).unsqueeze(0)\n",
        "\n",
        "    # Generate autocompletions using beam search\n",
        "    output_sequence = input_tokens[:]\n",
        "    for _ in range(max_length):\n",
        "        output = model(input_tensor)\n",
        "        _, topk_indices = torch.topk(output, beam_width, dim=1)\n",
        "\n",
        "        beam_candidates = []\n",
        "        for idx in topk_indices[0]:\n",
        "            predicted_word = idx_to_word[idx.item()]\n",
        "            beam_candidates.append(output_sequence + [predicted_word])\n",
        "\n",
        "        scores = score_beam_candidates(beam_candidates)\n",
        "        topk_scores, topk_indices = torch.topk(scores, beam_width)\n",
        "\n",
        "        output_sequence = beam_candidates[topk_indices[0].item()]\n",
        "        input_tensor = torch.tensor([word_to_idx[word] for word in output_sequence], dtype=torch.long).unsqueeze(0)\n",
        "\n",
        "    autocompletion = ' '.join(output_sequence)\n",
        "    print(f\"Suggestion: {autocompletion}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}